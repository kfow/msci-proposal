\documentclass{mprop}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

% alternative font if you prefer
% comment this out to go back
\usepackage{times}

% for alternative page numbering use the following package
% and see documentation for commands
%\usepackage{fancyheadings}


% other potentially useful packages
%\uspackage{amssymb,amsmath}
%\usepackage{fancyvrb}
%\usepackage[final]{pdfpages}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage[newfloat]{minted}
\usemintedstyle{emacs}
\usepackage{pgfplots}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{inconsolata}
\usepackage{graphics}
\pgfplotsset{compat=1.8}
\usepackage{tabu}
\newcommand{\code}[1]{\texttt{#1}}
\newenvironment{codelisting}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Code Sample}

% Use “\cite{NEEDED}” to get Wikipedia-style “citation needed” in document
\usepackage{ifthen}
\let\oldcite=\cite
\renewcommand\cite[1]{\ifthenelse{\equal{#1}{NEEDED}}{\ensuremath{^\texttt{[citation~needed]}}}{\oldcite{#1}}}

\renewcommand{\arraystretch}{1.5}

\begin{filecontents*}{data.csv}
name map length time
allterms 	0.4554	358.5  	2.51435
ne 			0.3979  93.45  	0.4132
tfidf 		0.3038  10  	0.13675
subject 	0.184  	9.75  	0.1578
\end{filecontents*}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Research Proposal: Smart Sensitivity Review}
\author{Kelvin Fowler}
\date{\today}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{intro}
The National Archives must release documents to the public after a certain amount of time. They must also respond to freedom of information requests. These are clauses of The Freedom of Information Act (2000)~\cite{foi}. Documents may be withheld or redacted if they are considered to be sensitive. There are clear guidelines in place which explain the reasons a document may be considered sensitive \cite{foiexemptions}. An example of sensitivity is the name of intelligence service informants or subjective negative sentiment towards foreign dignitaries, potentially harming international relations. In the digital age, more and more documents are digital borne, meaning that the process of reviewing all of these documents for sensitivities is becoming too cumbersome to keep up with the rate of document creation~\cite{allan2014record}. There is an increasing burden on archive facilities to decide ``what to keep''~\cite{moss2012have} as the collections of documents grow in size each day.

Part of this process involves comparing the contents of potentially sensitive documents to public domain documents~\cite{NEEDED}. If the content of the government documents is already in the public domain then these documents can be released without cause for concern. Currently the system in place to do this type of review is manual and insecure. It involves archivists using on-line search engines to retrieve public domain documents (such as news articles) in order to do the comparison. This means that potentially sensitive queries are being issued to public facing search engines, from a source in government. 

The digitization of sensitivity review presents an opportunity for novel technology based approaches to ease the burden. Information Retrieval (IR) is the process of providing information relative to a given information need. IR naturally lends itself to this task. The information need is \textit{public domain documents relating to a potentially sensitive document}. Indeed, IR has been applied in various ways to this task of sensitivity review (see Section~\ref{background_survey.sensitivity_review}. Sensitivity review is a classification task (that is, we want to specifically classify documents as \textit{sensitive} or \textit{not sensitive} and public domain knowledge identification is but one aspect of this process. Specifically, the identification of public domain knowledge aims to assist the manual review process, rather than to replace it. In fact, it has been noted that the manual review staff would be reluctant to allow technology to perform the task~\cite{gollins2014using}.

As a Level 4 project at the University of Glasgow, this task of identifying public domain knowledge was tackled in part. A Prototype UI and a REST API were produced. The system analyses documents to automatically generate queries to be run in a search engine in order to retrieve public domain documents. This project used natural language processing to tag named entities within the documents and generate queries from this data. This system of query generation was evaluated to find the best method. The evaluation led to conclusions and suggestions for directions of future work. These serve as an indication for the areas that we plan to investigate in this project.

This project seeks to extend upon the system built last year. We aim to improve the performance of these automatically generated queries significantly, through various techniques which will be discussed below. These new features will be evaluated empirically in order to prove their effectiveness.

\subsection{Terminology}
From henceforth a document from which queries are generated will be called a \textbf{Source Document}. A document which is to be retrieved by the search engine will be called a \textbf{Target Document}.
Generally, Source Documents will be potentially sensitive government documents which are to be reviewed. Target Documents will be public domain documents such as news or Wikipedia articles.
Analogously, the set of all Source Documents will be referred to as the \textbf{Source Collection} and a set of Target Documents will be referred to as a \textbf{Target Collection} (There are several distinct target collections).

\subsection{Structure}
This research proposal will take the following structure. In Chapter \ref{problem_statement} we will more formally define the problem this research seeks to address, and how this research will contribute to the field of information retrieval. In Chapter \ref{background_survey} related literature is examined in order to  the potential projects relevance. Chapter \ref{proposed_approach} outlines the proposed approach the research will take, as well as detailing some preliminary investigations into the feasibility of these goals. This includes motivations for choices of technologies. Finally, Section \ref{work_plan} highlights the deliverables of the project and potential dates for their completion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statement of Problem}\label{problem_statement}
With some motivation in mind for the problem this research might address, we must now consider how to formalize our aims. As will be discussed in more detail in Section \ref{background_survey} the previous project began by addressing the Named Entities latent in Source and Target documents. Named Entities are words or phrases referring specifically to people, places or things. These named entities were identified in both source and target documents so that they could be directly matched during retrieval. Unfortunately, the approaches attempted did not extend far beyond this. Specifically plans for future work mentioned date analysis and application of machine learning techniques. Both of these will be discussed in detail throughout Sections \ref{background_survey} and \ref{proposed_approach}.

However, we cannot be so specific in our description of the problem statement at the moment. We must remain sufficiently general so as not to exclude any potential avenues of research which could prove fruitful. As such, the following research questions perhaps best describe the goals of the proposed research.
\subsection{Research Questions}
\begin{enumerate}[label=\textbf{RQ.\arabic*}]
\item \textit{What information within source documents is most important to form a good query for retrieval of relevant public domain documents?}

This has already been tackled in part. We have shown last year that named entities extracted from source documents provide a good base query from which to perform retrieval. However it was also identified that by removing much of the information and context from the source document we lost some search accuracy (see Section~\ref{background_survey.previous_project} for more detail). Temporal information is present in the source documents and since this challenge often deals with specific events, this could prove very useful. We must also discover what other information is latent within source documents. Context surrounding Named and Temporal Entities could be important. The challenge remains to specifically identify these ``important'' pieces of context.

\item \textit{Can a comprehensive retrieval model be produced to generate queries from unseen documents and retrieve accurate and relevant public domain documents?}

This addresses a potentially more difficult question. We are able to extract features from source documents fairly easily (using existing NLP techniques and libraries) however combining these features and integrating them into a fully functioning retrieval system is a different problem. What is the most important? How do we keep queries short and performant? These are very important considerations.
\end{enumerate}

These research questions allow us freedom to investigate multiple avenues of query generation and retrieval techniques. At the end of Section~\ref{proposed_approach} we give more detailed hypotheses upon which to evaluate our proposed work.

% \subsection{Old Research Questions}
% \begin{enumerate}[label=\textbf{ORQ.\arabic*}]
% \item What is the most appropriate way to integrate and apply recognition of temporal entities into the query generation process and retrieval model, so as to improve the effectiveness of this search system?
% \item How can we use the machine learning to improve the effectiveness of our information retrieval system?
% \item Are there any other methods which measurably improve search effectiveness?
% \item Can this task be improved through identifying source document focus time?
% \item Can we directly compare source and target documents (including a temporal aspect) to retrieve target documents?
% \item Does including temporal data in query formulation improve search results?
% \item Does query expansion through knowledge base linking improve search results?
% \item Can Learning to Rank (L2R) techniques be used effectively in this problem domain?
% \item How else can we formulate queries from long documents?
% \end{enumerate}

\subsection{Not Included in this Project}
In the evaluation process of the previous project it was noted that temporal data could be leveraged to improve results. It was also in the original specification to include some machine learning (or learning to rank) features in the final product. This ended up being out of the scope of the project. As such, the work done last year can be seen as a skeleton or starting point to be used to experiment with new features this year. There is no intention to focus on a UI this year. While that was an interesting challenge to think about, efforts are better expended improving the performance of the underlying retrieval task. Once this is optimal, a UI can be devised.

\subsection{Further Challenges}
This project will deal with an existing set of off-line target documents. However this is, of course, not all of the public domain information available. Effectively, everything on the web is public domain information and thus, a fully functioning version of this application should be able to search the full web. This is actually part of the problem this research tries to address. Previously archivists use insecure web search to find out what was public domain knowledge.

\subsection{Contributions}
This research will contribute an improvement to the understanding of the formation of queries from source documents. Specific techniques for query generation from documents will be described and empirically evaluated. This research has applications in many areas of information retrieval outside of sensitivity review (for example, finding related web pages, news stories or other automatic recommendations).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BACKGROUND SURVEY/ LIT REVIEW
\section{Background Survey} \label{background_survey}
\subsection{Previous Project} \label{background_survey.previous_project}
This project is hinged on the existence of the work completed last year as part of a Level 4 Project \cite{DissertationKelvinFowler}. This was very much an investigatory project into the domain of using information retrieval to assist sensitivity review. Much focus was given to the interface through which the IR results could be displayed. This proposed project will not at all focus on the interface, but rather on the underlying IR technology that makes this possible.
The previous project began to address this problem. The key findings surrounded the effectiveness of Query Generation, that is, taking a Source Document and parsing from it a query which could be performed in the IR Search Engine, Terrier. More than just query generation it must be noted that for matching to succeed we must fundamentally alter our target collection in the process of indexing it.

\subsubsection{Methods}
\paragraph{Indexing}
Information Retrieval relies on indexing of documents for retrieval. This indexing step allows us to iterate through terms in a document, analyse them in some way and store them. This provides an ideal opportunity to run the natural language processing step necessary to extract named entities from text. These named entities could then be tagged in a specific way. In Ad-Hoc retrieval, the standard IR retrieval task, a user enters a query and a retrieval model returns documents to the user. Here we do not have specific user generated queries, but rather, entire documents from which queries must be generated. We must therefore also run these source documents through some indexing or preprocessing step in order to identify the terms from which the query will be generated.
In the previous project this indexing step was generalised in such a way that the same analysis could be run on both source and target documents, with the extra step of generating the required queries for source documents.

\paragraph{Query Formulation}
After indexing the source documents and identifying the named entities four queries were generated and written directly to the source document files. In the web app these files would have to be loaded in order to present the source document to the user, so there is no additional cost to storing the queries in these files.
Four queries were generated:
\begin{itemize}
  \item \textit{All Terms Query} - All terms, except stop words, were kept for this query. Named entities were identified in order to match the indexed target collection.
  \item \textit{Named Entities Query} - This query consisted of all the named entities in the document and nothing else. This mean the query was fairly short, however it did not contain any additional context. Named entities were extracted using Stanford Core NLP~\cite{StanfordNLPManning-EtAl2014}. Persons, Locations and Organisations were extracted.
  \item \textit{Tf-Idf Named Entities Query}, the named entities were given a Tf-Idf ranking and the top ten were used a query. Tf-Idf is a commonly used IR technique which attempts to estimate term importance through measures of how frequently the term appears. We obtained term frequency from the source document for each named entity and then document frequency from the all the source documents in that collection.
  \item \textit{Subject Query} - The subject line from a source document was used a query after having its named entities identified. This was an attempt to add context to the queries, as well as being short and easy to compute.
\end{itemize}

\subsubsection{Evaluation and Findings}
It was found that the performance of the \textit{All Terms Query} was the objectively the best, but the query took far too long to perform. The shorter \textit{Named Entities Query } was therefore chosen as the most effective considering all factors. Results can be seen in Table~\ref{standard_results}. Figure~\ref{mapgraphs} also shows the clear correlation between query length and processing time, as well as illustrating the point that a significantly shorter (and therefore faster) query can perform reasonably well.

\begin{center}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Query					& MAP 				& Mean Query  		& Mean Query 			& Mean Reciprocal 		& Mean Precision 	\\ 
						& 					& Length (Words) 	& Processing Time (s) 	& Rank 					& at 4				\\	\hline
All Terms             	& \textbf{0.4554} 	& 358.5             & 2.51435               & \textbf{0.8500}       & \textbf{0.6750} 	\\	\hline
Named Entities 			& 0.3979 			& 93.45    			& 0.4132 				& 0.7526				& 0.5875 			\\	\hline
Tf-Idf Named Entities 	& 0.3038 			& 10                & \textbf{0.13675}      & 0.7236       			& 0.4375 			\\	\hline
Subject               	& 0.1840 			& \textbf{9.75}     & 0.1578                & 0.3367       			& 0.2500 			\\	\hline
\end{tabular}%
}
\caption{Results of Offline Evaluation, Adapted from Level 4 Project~\protect\cite{DissertationKelvinFowler}}
\label{standard_results}
\end{table}
\end{center}

\begin{figure}[H]
\centering
\begin{subfigure}[t]{.5\textwidth}
\centering
\begin{tikzpicture}
 \begin{axis}[
 	ylabel=$MAP$,
    xlabel={Mean Query Processing Time},
    width=0.95\linewidth,
    height=0.5\linewidth
    ]
        \addplot table[x=time,y=map] {data.csv};
    \end{axis}
\end{tikzpicture}
\end{subfigure}%
~
\begin{subfigure}[t]{.5\textwidth}
\centering
\begin{tikzpicture}
 \begin{axis}[
 	ylabel=$MAP$,
    xlabel={Mean Query Length},
    width=0.95\linewidth,
    height=0.5\linewidth
    ]
        \addplot table[x=length,y=map] {data.csv};
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\caption{Analysis of Mean Average Precision (MAP) Scores against Query Processing Time and Query Length. Adapted from Level 4 Project~\protect\cite{DissertationKelvinFowler}} \label{mapgraphs}
\end{figure}

Other noteworthy findings include the fact that Named Entities of the Person type seem to have the biggest bearing on retrieval performance of the 3 named entity types used. Also, the extra context present in the All Terms Query clearly helped its performance. A key takeaway for this research is the challenge of how best to optimize keeping performance while including enough context around named entities.
The previous project also identified that temporal information could prove helpful as well as implementation of some kind of machine learning system.

\subsection{Sensitivity Review} \label{background_survey.sensitivity_review}
Sensitivity review is open challenge in information retrieval, and is being actively tackled, mainly by a team here at The University of Glasgow. Most of the work in this topic has been in the realm of automatically classifying sensitivities in documents or sensitive documents. McDonald et al \cite{mcdonald2014towards} use features such as mentions of specific countries and people as indication of sensitivity in a given document. More recently, McDonald et al \cite{mcdonald2017enhancing} apply machine learnings techniques (specifically semantic relationship recognition through word embeddings) to further assist the sensitivity classification process. This work does not seek to directly classify documents as sensitive, but rather provide a system through which manual sensitivity review can take place. The comparison to public domain documents is a difficult manual task, and it has been noted that archivists are reluctant to trust technology entirely \cite{gollins2014using}.

\subsection{Technology Assisted Review} \label{background_survey.tech_assisted_review}
More generally, sensitivity review falls into the category of Technology Assisted Review. Other applications of technology assisted review are E-Discovery \cite{oard2013information}, where the retrieval task is retrieving all relevant documents relating to the opposing party in a civil litigation case. This type of technology assisted review has also made headlines in recent years when the FBI used technology to assist their review of the Hillary Clinton E-Mails \cite{cnnclinton}.

\subsection{Temporal Information Retrieval} \label{background_survey.temporal_ir}
Some work has been done in the field of incorporating temporal data into IR systems. In Section~\ref{proposed_approach} we discuss some ways in which temporal information could be applied to this research. Most relevant here are \cite{TemporalBerberich2010,TemporalStrotgen2012}. Specifically Strötgen et al~\cite{TemporalStrotgen2012} describe a method of identifying top relevant temporal expressions in documents. That is, identification of which temporal expressions are of most import in a document. One can also check which temporal expressions are of most import in relation to a given query (or corpus). This has clear applications to this research in two ways. The first being the ability to rank temporal expressions for the purpose of query generation. The second being query based features which can be used to retrieve relevant documents (those with more relevant temporal expressions are potentially more relevant). Perhaps more directly relevant is the work of Berberich et al~\cite{TemporalBerberich2010} which deals specifically with temporal expressions in queries. They propose a language model which allows one to perform queries with both a temporal and textual component, and further proposes a method to combine these seemingly disparate retrieval measures, one of the open questions posed in a review of temporal information in IR\cite{alonso2011temporal}. Somewhat related to \cite{TemporalBerberich2010} the Language Model pioneers Li and Croft display in \cite{li2003time} a "time based language model" which promotes in search results documents from a given time period. Jatowt et Al~\cite{jatowt2013estimating} propose a method to estimate the time that a document focusses on. If we know this about a source document then we can apply the methods from \cite{li2003time} to give documents from that time period more weight.
This research would seek a novel approach combining all of these methods to utilize the temporal information latent in both source and target documents (not to mention knowledge of document creation times).

\subsection{Complex Queries}
Previously, we merely formed a query out of plain text and ran it through Terrier. Another option is to use a complex query consisting of formatted subqueries. Indri~\cite{strohman2005indri} is an existing search engine with this type of feature
The Indri query language provides a format for representing queries with multiple probabilistic features and specific logical combinations. A precursor to Indri was the Inquery search engine, which provided the basis for complex queries in a probabilistic retrieval model~\cite{Callan_theinquery}

\subsection{Other Uses of Machine Learning in IR}
In~\cite{GeneratingQueriesLee12, VerboseXue2010} machine learning is used to effectively trim long queries, only keeping the learned best chunks of each query. This is done not using a gold standard training set of the best chunks from a section of text, but rather learns the importance of chunks by performing retrieval with them.
Similarly Bendersky et al~\cite{LearningImportanceBendersky2010} learn the specific importance of terms within a query. They propose a method which is an extension the naive methods usually used to split a query into component terms (i.e. Term Frequency - Inverse Document Frequency). This is done through a combination of features, some based on other information found within the query as well as without, through use of external information sources.

\subsection{Knowledge Base Linking and Query Expansion} \label{background_survey.knowledge_base}
Query expansion is commonly used IR technique which allows one to extend an existing query with additional terms. Often this is done using Pseudo-Relevance Feedback, which involves extracting terms from the set of top retrieved documents from a query and adding them to the query to return yet more results. This has been well examined in papers such as~\cite{cao2008selecting,yu2003improving}

Another method which has proven effective for query expansion is knowledge base linking. Knowledge bases are large collections of information which are searchable. They contain named entities, events, dates and other terms along with additional information. They can generally be accessed programmatically. Dalton et Al \cite{KnowledgeBaseDalton2014} exemplify the use of this technique through extracting entities and linking them to knowledge base information to expand queries. They show it has considerable performance improvements over existing query expansion techniques.

\subsection{Others} \label{background_survey.others}
The end goal is a system which can identify related public domain documents for a source document from the set of all public domain documents. This means retrieval will have to take place on many, disparate target document collections (or potentially even a web search). In \cite{ResourceSelectionSi2002} a method is proposed to effectively select the best resource collection for retrieval based on a query. Although it is unlikely we will deal with this problem in this research it is worth noting that this is an issue which has some existing research.

% \subsection{Source Reference Table}
% This section gives a summarised reference view of the points made above, summarising relevant work, noting it's omissions and highlighting it's relevance to this work. The references will be listed in the order they appear above.

% \begin{tabu} to 1.1\textwidth { | X[l] | X[l] | X[l] | X[l] | }
% \hline
% Work Name and     Reference& Summary& Relevance to This     Work& Shortcomings \\ 
% \hline
% Kelvin Fowler Identifying Public Domain Knowledge \cite{DissertationKelvinFowler} & Investigation into using IR to retrieve public domain documents based on a source document. Queries were formed from the content of the source document and evaluated based on MAP and other common scores. A UI was created to help investigate how this task could be improved for the manual archivists working on sensitivity review.  & Direct predecessor to this proposed work. Provides a codebase upon which this work can be built. Identified areas to expand upon. & Only covered named entity tagging  and disregarded other information latent in source and target documents. Did not cover temporal information or attempt to apply information retrieval to the task.\\ \hline
% McDonald et al Towards a Classifier for Digital Sensitivity Review\cite{mcdonald2014towards} & & & \\ \hline
% McDonald et al Word Embeddings \cite{mcdonald2017enhancing} & & & \\ \hline
% Berberich Query Splitting\cite{TemporalBerberich2010} & & & \\ \hline
% Strotgen Identification of Top Relevant Temporal Terms\cite{TemporalStrotgen2012} & & & \\ \hline
% Kuzey \cite{TemporalKuzeyEtAl2016} & & & \\ \hline
% Croft and Li Time Based Language \cite{li2003time} & & & \\ \hline 
% Jatowt Estimating Document Focus Time\cite{jatowt2013estimating} & & & \\ \hline

% \hline
% \end{tabu}

% TODO: Could you mention information you found in IR Text books --- Not a terrible idea.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}\label{proposed_approach}
A basis exists already for this project in the form of the previous years software. We once again plan to use the Terrier \cite{macdonald2012puppy} search engine for the IR workload of this project. Terrier was developed and maintained at the University of Glasgow, so the local knowledge is invaluable. The developer also has extensive background with Terrier and so it makes sense to continue to use this search engine. We must however build a system on top of Terrier to perform the IR task at hand. In the subsequent sections we will discuss the various facets of this system and a proposed approach to implement each as well as some investigation of feasibility and some preliminary testing. To conclude we discuss our proposed methods of evaluation.

\subsection{Improved Named Entity Extraction}
The most obvious and immediate thing to do would be to try and improve the named entity extraction techniques which exist already. Perhaps the easiest fix would be to identify n-gram named entities as a single named entity. Previously a name like ``Jerry Seinfeld'' would be tagged as~\code{jerry\_person seinfeld\_person}. Better performance may be yielded through~\code{jerry\_seinfeld\_person}. Other improvements may be to surround the named entities with some additional context. Named entity extraction will again be performed using Stanford CoreNLP~\cite{StanfordNLPManning-EtAl2014}. In Section~\ref{proposedapproach.complexquery} we discuss how this 2-gram named entity can be effectively incorporated into a complex query.

\subsection{Temporal Tagging and Matching}
Another feature to easily extract is specific temporal information. We can do this through similar technology to the named entity tagger.
There are several temporal taggers in existence which could be integrated into the system. Since the project currently uses StanfordNLP~\cite{StanfordNLPManning-EtAl2014} we can easily integrate SUTime~\cite{chang2012sutime}, the temporal tagger included in StanfordNLP into the project. In a similar way to how named entity tagging works already, we can tag temporal entities also using an underscore. But how do we get them to match...? (Need to index the target collection also?)
There are several other choices of temporal taggers. Notably we have Heideltime~\cite{TemporalKuzeyEtAl2016}. However these are compared in~\cite{chang2012sutime} which shows SUTime to be the best performer.
It is important however to check that this is true for our own needs also. As such, some rudimentary tests were performed on to compare the effectiveness on date resolution of Heideltime and SUTime.

\subsubsection{Temporal Tagging Comparison}
In order to compare SUTime and Heideltime, 10 source documents were selected which contain relative references to time (e.g. ``\textit{yesterday}'', ``last \textit{month}''). These two temporal taggers were chosen for comparison after review of some literature indicating they had very close or equivalent performance. Some studies find SUTime preferable, whereas others indicate that Heideltime performs better. Both have their flaws and strengths in different scenarios.

In the comparison, the TimeX3 output of both taggers was reviewed manually. We wished to compare the taggers on their performance of specifically identifying discrete time references in text. To clarify, sometimes terms such as \textit{``currently''} resolved to a \code{PRESENT\_REF}. This is not particularly insightful or helpful and so examples such as this were completely ignored. Further, both SUTime and Heideltime contain facilities to identify sets or ranges of times. Again these were ignored in favour of explicit references. An example of one of these specific references is: 
\\ \code{<TIMEX3 tid=``t1'' type=``DATE'' value=``2008-05-01''>Thursday</TIMEX3>}.
This document was created on 2008-05-01, hence the specific resolution of ``Thursday'' to the observed date.
In the manual review we identified True Positives, False Positives, True Negatives and False Negatives. True Positives were correctly identified specific times. False Positives were when explicit times were identified incorrectly (e.g. ``the same day'' being resolved to the document creation date, rather than the date which was previously mentioned in the same sentence. Heideltime identified this correctly, while SUTime did not). True Negatives were all terms which did not refer to dates that the taggers correctly skipped. False Negatives were when the tagger missed an explicit reference to time (e.g. ``2007/08'' being missed due to the ambiguity of the slash.)
Both taggers resolve to TimeML's TimeX3\cite{timeml} mark-up language which gives a precise format to time annotation.

\paragraph{Heideltime Configuration}
Heideltime contains a feature which allows the developer to choose between narrative and news style documents for annotation. It was not clear which of these would be better for our source collection. Since the target collection was Associated Press articles, it seems obvious to use the news annotation style. Both options were tried, however using the narrative style meant that the document origin time was ignored, which meant dates were not resolved as accurately for source documents. Thus the news style was used throughout.

Both taggers required minimal effort to integrate with our existing system. In fact, SUTime was included as part of the dependencies required for our existing system of named entity recognition. You can see the amounts of identification were counted, the True Negative amount being calculated through subtraction of the sum of the other values from the total term count of each document. Subsequently, the Accuracy and the Standard F-Measure (\textbf{$ F_1 $}) were calculated and are reported below in Table. \ref{temporalcomparison}.
In \ref{temporalcomparison} G represents good, usable tagged dates. B represents Wrong, Unusable or Unhelpful tagged dates. ? represents tagged dates that are potentially helpful, but are not immediately obvious.
\todo{mcnemars test}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
& \multicolumn{4}{|c|}{SUTime}    & \multicolumn{4}{|c|}{Heideltime} \\ 
\cline{2-9}
Document & TP  & FP  & TN   & FN    & TP & FP & TN   & FN    \\ \hline
1        & 11  & 1   & 374  & 0	    & 12 & 0  & 374  & 0     \\ \hline
2        & 5   & 0	 & 788  & 0	    & 5  & 0  & 788  & 0     \\ \hline
3		 & 3   & 0   & 228  & 0		& 3  & 0  & 228  & 0	 \\ \hline
4        & 1   & 1	 & 70   & 0		& 2  & 0  & 70   & 0	 \\ \hline
5	     & 2   & 0	 & 64   & 0		& 2	 & 0  & 64   & 0     \\ \hline
6		 & 14  & 0	 & 285  & 2		& 15 & 0  & 285  & 1     \\ \hline
7		 & 12  & 0	 & 389  & 0		& 11 & 0  & 390  & 1	 \\ \hline
8		 & 2   & 0	 & 71   & 0	  	& 2	 & 0  & 71   & 0	 \\ \hline
9		 & 9   & 0	 & 287  & 0		& 9	 & 0  & 287  & 0	 \\ \hline
10		 & 7   & 1	 & 272  & 0		& 8	 & 0  & 271  & 0	 \\ \hline
Total    & 66  & 3	 & 2828 & 2	  	& 69 & 0  &	2828 & 2     \\ \hline
F1-Score & \multicolumn{4}{|c|}{0.9635} & \multicolumn{4}{|c|}{0.9928} \\ \hline
Accuracy & \multicolumn{4}{|c|}{0.9982} & \multicolumn{4}{|c|}{0.9997} \\ \hline
\end{tabular}
\caption{Heideltime vs. SUTime on Sample Source and Target Documents}
\label{temporalcomparison}
\end{table}
Both display high measures of performance, however Heideltime appears to better for our purposes. Through the course of the manual review of the data this can be confirmed. Heideltime seemed more close to way a human would identify dates. The date ranges that Heideltime identifies could also be helpful in the future of this project.

\subsubsection{Using Temporal Tagging}
Although we plan to use Heideltime for temporal tagging of source and target documents, we must now understand how we can incorporate this into the retrieval model.
As discussed in \ref{background_survey} there are several existing approaches using temporal matching that we could implement and extend. \cite{} propose a method for estimating document focus time. Combined with the work in \cite{li2003time} we could more heavily weight documents which were created on (or near) this focus time. Another approach is to directly compare the temporal entities in a source document with those in target documents, as is discussed in \cite{makkonen2004simple}. We could attempt to improve upon their temporal similarity vector approach, which they concede is not excellent.

The approach should therefore be to implement these various features and learn a model in Terrier which applies this (in various ways) to ranking, using learning to rank. These combinations of ranking models can then be compared in order to decide which use of temporal information is the best.

\subsection{Better and More General Query Generation}
We can see from \cite{GeneratingQueriesLee12} that various methods can be employed to turn a piece of text into a query. Not only must we identify named entities and temporal data (see above), but we must also consider other n-grams within the text. For example from the text "Hillary Clinton was today campaigning to become the democratic presidential nominee." we can produce a query of the named entities: "hillary\_person clinton\_person democratic\_organization". Clearly, however this lacks much of the context from the original text. A more realistic query to form from this text would be "hillary\_clinton\_person democrat\_organization nominee campaign". This contains far more of the relevant data in the original text. Although we emphasize the role of temporal data in this research proposal, it must not be overlooked that in order to have a functional and performant retrieval system other features like named entities must be taken into account. We can continue to use the named entity indexing step from the previous project as detailed in \cite{DissertationKelvinFowler}.

\subsection{Combining Query Information to Create a Complex Query}
\label{proposedapproach.complexquery}
A prototype version of Terrier v.5 has been provided for this project. This allows the use of some of the features of the Indri Query Language. The Indri Query Language provides specified syntax to produce structured complex queries. These queries can be parsed and actioned appropriately by a search engine, such as Terrier.

This has clear applications to our problem domain. One can take a given source document and extract from it Named Entities (Persons, Locations, Organisations), Explicit Time Stamps and Other Chunks.

Indri, however requires our target documents to be in a specific format. This means, in a similar vein to the previous years project we must index and alter all of the target documents in a specific way, in order to pick up the correct terms and entities.

Some considerations for this type of complex query are the fact that often surnames are used a synonyms for full names. This could be accounted for by including extracting the final word from n-gram person named entities and including that in the query. Another is how to use the time information discussed above. Initially it seems reasonable to just match on exact dates, however we could implement more general features that examine time intervals. Finally we have a lot of other information latent in a document which is neither temporal references nor named entities. This can be included in the complex query as is and weighted appropriately.

\begin{codelisting}
\begin{minted}[breaklines,breaksymbolleft={}]{java}
#1{Hillary Clinton}
#1{Bill Clinton}
#1{United States}
\end{minted}
\captionof{listing}{A Sample of a Query Using the Indri Query Language}
\label{code:indrisample}
\end{codelisting}

\subsection{Learning to Rank}
With this proposed retrieval model in mind, leveraging IndriQL~\cite{strohman2005indri}, we now have a good set of features which we can use to learn a model for retrieval.
IndriQL provides us with the ability to give a weight to different parts of query, however this has not directly been implemented in Terrier. Instead we can use the existing learning to rank technology in Terrier to automatically weight query sections or sub queries in the most appropriate way.  We hypothesize for example that the named entities in a source document will be more important that the time references. Thus they should be weighted more heavily in the retrieval model. L2R can do this automatically assuming we give it appropriate features from which to calculate a weighting model.
\begin{enumerate}[label=\textbf{F. \arabic*}]
\item Named Entities
\item Time References
\item Other Chunks
\end{enumerate}

\subsection{Knowledge Base Linking and Query Expansion}
Query expansion is another popular IR technique that was not touched upon in the previous work. Knowledge base linking can be used successfully in conjunction with query expansion to include in queries well defined references to specific events or people. An example of how this could help is by resolving a general term like "The President" to a more specific reference to say, "Barack Obama". This is analogous to the resolution of specific times as mentioned above, however comes with the extra step of spatial awareness in the document. We can only resolve this example if the context of the document is the USA. This presents an interesting challenge in itself.

\subsection{Evaluation and Experimental Set Up}
\subsubsection{Hypotheses}
We can form some hypotheses in response the research questions proposed in Section~\ref{problem_statement}. These hypotheses allows us to design and plan efficient experiments for evaluating the effectiveness of our proposed IR system.

\begin{enumerate}[label=\textbf{Hyp.\arabic*}]
\item Temporal Information will be less important to complex queries than named entities and other terms.
\item Adding context to queries will improve performance.
\item This complex query system will see an improvement over the query formulations of last year.
\item Learning to rank will identify features of the complex query to weight more or less heavily.
\end{enumerate}

\subsection{Effectiveness Measures} \label{evalmeasures}
In order to evaluate the effectiveness of the retrieval system in relation to the above hypotheses we must employ formal measures of comparison.
Information retrieval is usually measures in terms of \textit{precision} and \textit{recall}. \textit{Precision} is the measure of how many of the retrieved documents are relevant, whereas \textit{recall} is a measure of how many of the total number of relevant documents were retrieved.
The most important and ubiquitous measure is likely \textit{Mean Average Precision} (MAP). MAP is commonly used in experiments involving TREC collections as it gives a good objective overview of the performance of given system and is well understood IR research communities.
\textit{Precision at 5/10} (P\@5/10) This measure allows us to measure the precision at the first n documents returned. That is, we can see a measure of how effective the system is a retrieving and ranking documents close to the top.
\textit{Mean Reciprocal Rank} (MRR) allows us to measure in a list of ranked results how close the top of the list the first relevant result was. The closer to the first rank the better.

As this is an applied study, with a real life application it is prudent to also investigate the length of time it takes to run queries with different configurations. In the previous years project~\cite{DissertationKelvinFowler} the \textit{All Terms Query} was regarded as too slow to justify it's use in an interactive user environment.

\paragraph{Test Collection}
For evaluation we use a collection of Associated Press articles from the TREC 1 Ad-Hoc task~\cite{trecnist}. The documents are a collection of Associated Press articles. These are indeed public domain documents and so are representative of the documents retrieved in the sensitivity review process.

\subsubsection{Using Existing Relevance Judgements} \label{existingrelevance}
In order to ensure using existing topics and relevance judgements is a reasonable thing to do for this project this approach was prototyped.
Last years results were compared with results using the new test collection to ensure we retrieved comparable results. A detailed explanation can be seen below.

Drawing inspiration from Lee et al~\cite{GeneratingQueriesLee12} we can also use the collection of target documents as representative source documents. In order to this we must look specifically at the existing topics and relevance judgements provided for the TREC 1 Ad-Hoc task. If we take a topic, we cant take a selected relevant document for this topic and use this document from which to generate queries. Provided the document is analogous in some way to the original query then the information need of ``public domain documents relating to this document'' is fulfilled by the relevant documents according to the qrels for the original topic.
We can form an effective test collection by choosing a number \code{N} of topics. For each of these topics we can choose a random relevant document according to the corresponding qrel entry. We must then check if the document is representative of the initial topic (as in it is representative of the description in the trec topics file). Then this document can be added to the source collection.
Note that this method means we must ignore the source document if it appears in the result set for a query, since it is likely to appear and must remain in the target collection for other queries.

This would not be a suitable substitution in other sensitivity review scenarios, but we are not considering document sentiment here, but rather the objective terms and phrases mentioned in a document, which are similar across source and target documents in practice (e.g. named entities, temporal references and noun phrase chunks).

This approach is not void of issues and it is likely a manual test collection will have to be produces for thoroughness and accuracy in due course.

\subsection{Summary}
Thus, we have outlined a specific approach we might follow in order to implement and evaluate a solution to the proposed problem. We must effectively and efficiently extract relevant data from source documents. This data can be composed into a complex query to run in Terrier against the collection of target documents. We can use learning to rank to automatically weight the different sections of the query in a somewhat optimal way in order to improve performance on unseen documents. We can then evaluate this approach using existing documents collections as a test collection, in order to demonstrate the effectiveness of the new system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   WORK PLAN  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Plan} \label{work_plan}
With the above proposed approach in mind we can set some definitive milestones for this research project.

\subsection{Query Components}
We should begin by implementing a system capable of reading documents and extracting the relevant information. That is, Named Entities of different classes (Persons, Locations, Organisations), Explicit Temporal Expressions and other Chunks (likely Noun Phrase Chunks).

\subsection{Complex Queries}
With these query components created we can now explicitly form or complex query, using the tools provided by Terrier v5 (IndriQL features). This means we can match exactly bigram named entities as well as matching temporal expressions as unigrams. We can also match the other chunks. We can give each of these parts of the query a label. We should do this in several different configurations to ensure we are not missing any important query formulations.

\subsection{Learning to Rank}
With the query formulations generated we must run these through the learning to rank system in Terrier in order to produce the appropriate weighting to give each part of the complex query. This should be done for all query configurations in order to have sufficient data for comparisons.

\subsection{Formulate Test Collection}
We can use the process described in Section~\ref{existingrelevance} to generate our source collection from which to generate queries. Ideally this source collection would be fairly large in order to produce a good amount of data for evaluation.

\subsection{Evaluation}
With the source collection created we can run experiments using Terrir in order to produce results in the measures described in Section~\ref{evalmeasures}. These experiments will be run on each query configuration along with it's calculated weights after learning to rank.

\subsection{Further Work}
Once all of the above has been completed and evaluated we can look forwards to improving the query generation stage even more. Some potential avenues are query expansion through knowledge base linking or allowing more generality in temporal comparisons. We may also formulate and additional test collection in order to perform addiontal evaluation to more concretely rate the performance of our system.

\subsection{Time Frames}
\begin{table}[H]\label{table:time-allocation}
\begin{center}
\begin{tabular}{r|l}
    \emph{Section} & \emph{Intended completion time}\\
    Generate Baseline and Effective Test Collection & Late December\\
    Comprehensive Query Generation and Learning to Rank & Mid January\\
    Evaluation and Experiments & February\\
    Other Enhancements & Early March\\
    Report Findings & End March\\
\end{tabular}\par
    \caption{Proposed Time Line for Completion of Work}
\end{center}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% it is fine to change the bibliography style if you want
\bibliographystyle{plain}
\bibliography{mprop}
\end{document}
